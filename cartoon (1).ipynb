{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartoon.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChJaItUvJl1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import functools\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ8iCgUYJp4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import PIL \n",
        "from PIL import Image\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQvBMra_Jp7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_again = True \n",
        "if load_again:\n",
        "  folder  = '/content/drive/My Drive/Datasets/face_comic.zip'\n",
        "  !cp \"{folder}\" .\n",
        "  !unzip -q face_comic.zip\n",
        "  !rm -r face_comic.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0unFytIrbG-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_gaussian_blur(img):\n",
        "  n = img.shape[0]\n",
        "  for i in range(n):\n",
        "    img[i]= cv2.GaussianBlur(img[i],(5,5),0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLDDwIffJp9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slice_idx = 1600\n",
        "path_dir = os.path.curdir\n",
        "face_dataset = np.load(path_dir+'/face.npy',allow_pickle=True)\n",
        "comic_dataset = np.load(path_dir+'/comic.npy',allow_pickle=True)\n",
        "train_face = face_dataset[:slice_idx]\n",
        "train_comic = comic_dataset[:slice_idx]\n",
        "test_face = face_dataset[slice_idx:2000]\n",
        "test_comic = comic_dataset[slice_idx:2000]\n",
        "train_face = train_face.reshape(-1,128,128,3)\n",
        "train_comic = train_comic.reshape(-1,128,128,3)\n",
        "test_face = test_face.reshape(-1,128,128,3)\n",
        "test_comic = test_comic.reshape(-1,128,128,3)\n",
        "apply_gaussian_blur(train_face)\n",
        "apply_gaussian_blur(train_comic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqR4gizIbYND",
        "colab_type": "code",
        "outputId": "9eba3bc5-5cb0-4f52-9ce0-9b149309eac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_comic.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 128, 128, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obxzJXg6Jp_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    This code is inspired from (https://github.com/jcjohnson/fast-neural-style)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n",
        "        assert(n_blocks >= 0)\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        model = [nn.ReflectionPad2d(3),\n",
        "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
        "                 norm_layer(ngf),\n",
        "                 nn.ReLU()]\n",
        "\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):  # add downsampling layers\n",
        "            mult = 2 ** i\n",
        "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
        "                      norm_layer(ngf * mult * 2),\n",
        "                      nn.ReLU()]\n",
        "\n",
        "        mult = 2 ** n_downsampling\n",
        "        for i in range(n_blocks):       # add ResNet blocks\n",
        "\n",
        "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
        "\n",
        "        for i in range(n_downsampling):  # add upsampling layers\n",
        "            mult = 2 ** (n_downsampling - i)\n",
        "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
        "                                         kernel_size=3, stride=2,\n",
        "                                         padding=1, output_padding=1,\n",
        "                                         bias=use_bias),\n",
        "                      norm_layer(int(ngf * mult / 2)),\n",
        "                      nn.ReLU()]\n",
        "        model += [nn.ReflectionPad2d(3)]\n",
        "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
        "        model += [nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"Define a Resnet block\"\"\"\n",
        "\n",
        "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = []\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU()]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward function (with skip connections)\"\"\"\n",
        "        out = x + self.conv_block(x)  # add skip connections\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-177MSi2JqCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLayerDiscriminator(nn.Module):\n",
        "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
        "\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        kw = 4\n",
        "        padw = 1\n",
        "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2)]\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2 ** n, 8)\n",
        "            sequence += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2 ** n_layers, 8)\n",
        "        sequence += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        ]\n",
        "\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmE7DVeIJqEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GANLoss(nn.Module):\n",
        "    \"\"\"Define different GAN objectives\"\"\"\n",
        "\n",
        "    def __init__(self, target_real_label=1.0, target_fake_label=0.0):\n",
        "\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def get_target_tensor(self, prediction, target_is_real):\n",
        "\n",
        "        if target_is_real:\n",
        "            target_tensor = self.real_label.to(device)\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "        return target_tensor.expand_as(prediction).to(device)\n",
        "\n",
        "    def __call__(self, prediction, target_is_real):\n",
        "      \n",
        "        target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
        "        loss = self.loss(prediction, target_tensor)\n",
        "    \n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0iIj29FJqG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYG6tQKWN085",
        "colab_type": "code",
        "outputId": "ed3bc241-5cd7-4281-a8b9-4caadd99ca76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCsmSepZJ6Pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# net = Net().to(device)\n",
        "gen_xy = ResnetGenerator(3,3).to(device)\n",
        "gen_yx = ResnetGenerator(3,3).to(device)\n",
        "disc_x = NLayerDiscriminator(3).to(device)\n",
        "disc_y = NLayerDiscriminator(3).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRhXx7TaJ6Zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer_g = optim.Adam(itertools.chain(gen_xy.parameters(),gen_yx.parameters()),lr=0.0002)\n",
        "optimizer_d = optim.Adam(itertools.chain(disc_x.parameters(),disc_y.parameters()),lr=0.0002)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36pLr1QzJ6jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "identity_l = nn.L1Loss()\n",
        "gan_l = GANLoss()\n",
        "cycle_l = nn.L1Loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs6uM_bjKEcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img):\n",
        "  # plot_image = img.view(3,128,128).numpy()*255.0\n",
        "  # plt.imshow(np.transpose(img,(1,2,0)))\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7cUvnO2KEhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(real_x,real_y):\n",
        "\n",
        "\n",
        "  # G_xy\n",
        "  optimizer_g.zero_grad()\n",
        "\n",
        "  fake_y = gen_xy(real_x)\n",
        "  prediction_y = disc_x(fake_y)\n",
        "  cycled_x = gen_yx(fake_y)\n",
        "\n",
        "  loss_id_y = identity_l(gen_xy(real_y),real_y)\n",
        "  loss_cyc_x = cycle_l(real_x,cycled_x)*10\n",
        "  loss_1 = gan_l(prediction_y,True) \n",
        "\n",
        "  loss_x = loss_id_y + loss_cyc_x + loss_1\n",
        "  loss_x.backward(retain_graph=True)\n",
        "  optimizer_g.step()\n",
        "\n",
        "\n",
        "  # G_yx\n",
        "  optimizer_g.zero_grad()\n",
        "\n",
        "  fake_x = gen_yx(real_y)\n",
        "  prediction_x = disc_y(fake_x)\n",
        "  cycled_y = gen_xy(fake_x)\n",
        "\n",
        "  loss_id_x = identity_l(gen_yx(real_x),real_x)\n",
        "  loss_cyc_y = cycle_l(real_y,cycled_y)*10\n",
        "  loss_2 = gan_l(prediction_x,True) \n",
        "\n",
        "  loss_y = loss_id_x + loss_cyc_y + loss_2\n",
        "  loss_y.backward(retain_graph=True)\n",
        "  optimizer_g.step()\n",
        "\n",
        "  # Discriminator loss\n",
        "\n",
        "  # d_x\n",
        "\n",
        "  optimizer_d.zero_grad()\n",
        "\n",
        "  pred_real = disc_x(real_y)\n",
        "  real_loss_d_x = gan_l(pred_real,True)\n",
        "\n",
        "  pred_fake = disc_x(fake_y.clone().detach())\n",
        "  fake_loss_d_x = gan_l(pred_fake,False)\n",
        "\n",
        "  loss_d_x = (real_loss_d_x+fake_loss_d_x)*0.5\n",
        "  loss_d_x.backward()\n",
        "  optimizer_d.step()\n",
        "\n",
        "  # d_y\n",
        "  \n",
        "  optimizer_d.zero_grad()\n",
        "\n",
        "  pred_real = disc_y(real_x)\n",
        "  real_loss_d_y = gan_l(pred_real,True)\n",
        "\n",
        "  pred_fake = disc_y(fake_x.clone().detach())\n",
        "  fake_loss_d_y = gan_l(pred_fake,False)\n",
        "\n",
        "  loss_d_y = (real_loss_d_y+fake_loss_d_y)*0.5\n",
        "  loss_d_y.backward()\n",
        "  optimizer_d.step()\n",
        "\n",
        "  # print(\"Losses: \",gen_loss.item(),disc_x_loss.item(),disc_y_loss.item())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DifLWRoEJqL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "two_data = list(zip(train_face,train_comic))\n",
        "train_x = torch.utils.data.DataLoader(two_data,batch_size=1,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytgtIOOPJqOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_val = torch.Tensor(test_face)\n",
        "test_val = test_val.type(torch.FloatTensor)\n",
        "test_val = test_val/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pIdJgHCPd_J",
        "colab_type": "code",
        "outputId": "17a31062-7314-41b6-fc19-02ddcba17940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_val.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([400, 128, 128, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuHrssbMhPj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_name_1 = 'gen_xy'\n",
        "model_save_name_2 = 'gen_yx'\n",
        "model_save_name_3 = 'disc_x'\n",
        "model_save_name_4 = 'disc_y'\n",
        "path1 = F\"/content/gdrive/My Drive/Models/cyclegan/face_comic/{model_save_name_1}\"\n",
        "path2 = F\"/content/gdrive/My Drive/Models/cyclegan/face_comic/{model_save_name_2}\"\n",
        "path3 = F\"/content/gdrive/My Drive/Models/cyclegan/face_comic/{model_save_name_3}\"\n",
        "path4 = F\"/content/gdrive/My Drive/Models/cyclegan/face_comic/{model_save_name_4}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRYb2auyhQfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_models():\n",
        "  gen_xy.load_state_dict(torch.load(path1))\n",
        "  gen_yx.load_state_dict(torch.load(path2))\n",
        "  disc_x.load_state_dict(torch.load(path3))\n",
        "  disc_y.load_state_dict(torch.load(path4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8_nfOi5hQXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_models():\n",
        "  torch.save(gen_xy.state_dict(),path1)\n",
        "  torch.save(gen_yx.state_dict(),path2)\n",
        "  torch.save(disc_x.state_dict(),path3)\n",
        "  torch.save(disc_y.state_dict(),path4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prI54lOVKZt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "  for i in range(40):\n",
        "    print(i)\n",
        "    for x,y in train_x:\n",
        "      # print(j)\n",
        "      x = x.type(torch.cuda.FloatTensor)\n",
        "      x.requires_grad_(requires_grad=True)\n",
        "      y = y.type(torch.cuda.FloatTensor)\n",
        "      y.requires_grad_(requires_grad=True)\n",
        "      x = x/255.0\n",
        "      y = y/255.0\n",
        "      real_x = x.view(-1,3,128,128).to(device)\n",
        "      real_y = y.view(-1,3,128,128).to(device)\n",
        "      train_step(real_x,real_y)\n",
        "\n",
        "    sample_output = gen_xy(test_val[75].view(-1,3,128,128).to(device)).to(torch.device(\"cpu\"))\n",
        "    sample_img = sample_output.detach().numpy().reshape(128,128,3)\n",
        "    imshow((sample_img*255).astype(np.uint8)) \n",
        "\n",
        "    if (i+1)%20==0:\n",
        "      save_models()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "089w0cgDKc7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = 75\n",
        "test_img = test_val[m].numpy()\n",
        "test_img = test_img.reshape(128,128,3)\n",
        "imshow(test_img)\n",
        "\n",
        "\n",
        "sample_output = gen_xy(test_val[m].view(-1,3,128,128).to(device)).to(torch.device(\"cpu\"))\n",
        "sample_img = sample_output.detach().numpy().reshape(128,128,3)\n",
        "imshow((sample_img*255).astype(np.uint8)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sI0Ko62KZ0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}